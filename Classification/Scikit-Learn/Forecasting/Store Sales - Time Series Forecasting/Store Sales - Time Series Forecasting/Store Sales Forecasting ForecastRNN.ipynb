{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Keras\n",
    "# ==============================================================================\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # 'tensorflow', 'jaxÂ´ or 'torch'\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping\n",
    "import skforecast\n",
    "from skforecast.ForecasterRnn import ForecasterRnn\n",
    "from skforecast.ForecasterRnn.utils import create_and_compile_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "\n",
    "# Warning configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_events_df = pd.read_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/holidays_events.csv')\n",
    "stores_df = pd.read_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/stores.csv')\n",
    "transactions_df = pd.read_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/transactions.csv')\n",
    "train_df = pd.read_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/train.csv')\n",
    "test_df = pd.read_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/test.csv')\n",
    "oil_df = pd.read_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/oil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_events_df = holiday_events_df.sort_values(by='transferred', ascending=False).drop_duplicates(subset='date', keep='first')\n",
    "holiday_events_df[holiday_events_df['transferred'] == True].head()\n",
    "holiday_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].bfill()\n",
    "oil_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = train_df.merge(stores_df, how='left', on='store_nbr').merge(transactions_df, how='left', on=['date', 'store_nbr']).merge(oil_df, how='left', on='date').merge(holiday_events_df, how='left', on='date')\n",
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_df = test_df.merge(stores_df, how='left', on='store_nbr').merge(transactions_df, how='left', on=['date', 'store_nbr']).merge(oil_df, how='left', on='date').merge(holiday_events_df, how='left', on='date')\n",
    "full_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-format the date column and set it as index\n",
    "full_train_df['date'] = pd.to_datetime(full_train_df['date'], format = '%Y-%m-%d')\n",
    "# Re-format the date column and set it as index\n",
    "full_test_df['date'] = pd.to_datetime(full_test_df['date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df['day_of_week'] = full_train_df['date'].dt.dayofweek\n",
    "full_test_df['day_of_week'] = full_test_df['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df['month'] = full_train_df['date'].dt.month\n",
    "full_test_df['month'] = full_test_df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to access 'date' column\n",
    "full_train_df.reset_index(inplace=True)\n",
    "full_test_df.reset_index(inplace=True)\n",
    "\n",
    "# Set the date as index\n",
    "full_train_df.set_index('date', inplace=True)\n",
    "full_test_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset frequency to be (D)aily data\n",
    "# full_train_df = full_train_df.asfreq('D', method = 'bfill') \n",
    "# full_test_df = full_test_df.asfreq('D', method = 'bfill') \n",
    "# Fill missing value with the latest available data\n",
    "\n",
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the type of products\n",
    "family_label_encoder = LabelEncoder()\n",
    "full_train_df['family_encoded'] = family_label_encoder.fit_transform(full_train_df['family'])\n",
    "full_test_df['family_encoded'] = family_label_encoder.transform(full_test_df['family'])\n",
    "\n",
    "#Encoding the citys and states\n",
    "city_label_encoder = LabelEncoder()\n",
    "full_train_df['city_encoded'] = city_label_encoder.fit_transform(full_train_df['city'])\n",
    "full_test_df['city_encoded'] = city_label_encoder.transform(full_test_df['city'])\n",
    "\n",
    "state_label_encoder = LabelEncoder()\n",
    "full_train_df['state_encoded'] = state_label_encoder.fit_transform(full_train_df['state'])\n",
    "full_test_df['state_encoded'] = state_label_encoder.transform(full_test_df['state'])\n",
    "\n",
    "#Encoding the type of stores\n",
    "type_store_label_encoder = LabelEncoder()\n",
    "full_train_df['type_store_encoded'] = state_label_encoder.fit_transform(full_train_df['type_x'])\n",
    "full_test_df['type_store_encoded'] = state_label_encoder.transform(full_test_df['type_x'])\n",
    "\n",
    "#Transactions fillna\n",
    "full_train_df['transactions'] = full_train_df['transactions'].fillna(0)\n",
    "full_test_df['transactions'] = full_test_df['transactions'].fillna(0)\n",
    "#Transactions Scale\n",
    "transactions_scaler = MinMaxScaler()\n",
    "full_train_df['transactions_scaled'] = transactions_scaler.fit_transform(full_train_df[['transactions']])\n",
    "full_test_df['transactions_scaled'] = transactions_scaler.transform(full_test_df[['transactions']])\n",
    "\n",
    "#Encoding the type of holidays\n",
    "type_holiday_label_encoder = LabelEncoder()\n",
    "full_train_df['type_y'].fillna('no_holiday', inplace=True)\n",
    "full_test_df['type_y'].fillna('no_holiday', inplace=True)\n",
    "combined_type_y = pd.concat([full_train_df['type_y'], full_test_df['type_y']])\n",
    "type_holiday_label_encoder.fit(combined_type_y)\n",
    "full_train_df['type_holiday_encoded'] = type_holiday_label_encoder.transform(full_train_df['type_y'])\n",
    "full_test_df['type_holiday_encoded'] = type_holiday_label_encoder.transform(full_test_df['type_y'])\n",
    "\n",
    "#Encoding the type of holidays\n",
    "locale_label_encoder = LabelEncoder()\n",
    "full_train_df['locale'].fillna('no_holiday', inplace=True)\n",
    "full_test_df['locale'].fillna('no_holiday', inplace=True)\n",
    "combined_type_y = pd.concat([full_train_df['locale'], full_test_df['locale']])\n",
    "locale_label_encoder.fit(combined_type_y)\n",
    "full_train_df['locale_encoded'] = locale_label_encoder.transform(full_train_df['locale'])\n",
    "full_test_df['locale_encoded'] = locale_label_encoder.transform(full_test_df['locale'])\n",
    "\n",
    "\n",
    "#Encoding the type of holidays\n",
    "transferred_label_encoder = LabelEncoder()\n",
    "full_train_df['transferred'].fillna('no_holiday', inplace=True)\n",
    "full_test_df['transferred'].fillna('no_holiday', inplace=True)\n",
    "full_train_df['transferred'].replace({True: 'transferred', False: 'not_transferred'}, inplace=True)\n",
    "full_test_df['transferred'].replace({True: 'transferred', False: 'not_transferred'}, inplace=True)\n",
    "combined_type_y = pd.concat([full_train_df['transferred'], full_test_df['transferred']])\n",
    "transferred_label_encoder.fit(combined_type_y)\n",
    "full_train_df['transferred_encoded'] = transferred_label_encoder.transform(full_train_df['transferred'])\n",
    "full_test_df['transferred_encoded'] = transferred_label_encoder.transform(full_test_df['transferred'])\n",
    "\n",
    "oil_scaler = MinMaxScaler()\n",
    "full_train_df['dcoilwtico'] = full_train_df['dcoilwtico'].bfill()\n",
    "full_test_df['dcoilwtico'] = full_test_df['dcoilwtico'].bfill()\n",
    "full_train_df['dcoilwtico_scaled'] = oil_scaler.fit_transform(full_train_df[['dcoilwtico']])\n",
    "full_test_df['dcoilwtico_scaled'] = oil_scaler.transform(full_test_df[['dcoilwtico']])\n",
    "\n",
    "sales_scaler = MinMaxScaler()\n",
    "full_train_df['sales_scaled'] = sales_scaler.fit_transform(full_train_df[['sales']])\n",
    "# full_test_df['sales_scaled'] = sales_scaler.transform(full_test_df[['sales']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=full_train_df[['store_nbr', 'onpromotion', 'cluster', 'day_of_week', 'month', 'family_encoded', 'city_encoded', 'state_encoded', 'type_store_encoded', 'type_holiday_encoded', 'locale_encoded', 'transferred_encoded', 'dcoilwtico', 'sales']]\n",
    "test_data=full_test_df[['store_nbr', 'onpromotion', 'cluster', 'day_of_week', 'month', 'family_encoded', 'city_encoded', 'state_encoded', 'type_store_encoded', 'type_holiday_encoded', 'locale_encoded', 'transferred_encoded', 'dcoilwtico']]\n",
    "\n",
    "# Ensure the date ranges are correctly defined\n",
    "train_data_start=pd.to_datetime('2013-01-01')\n",
    "train_data_end=pd.to_datetime('2017-08-15')\n",
    "\n",
    "test_data_start=pd.to_datetime('2017-08-16')\n",
    "test_data_end=pd.to_datetime('2017-08-31')\n",
    "\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the train_data by 'store_nbr' and 'family'\n",
    "grouped_train = train_data.groupby(['store_nbr', 'family_encoded'])\n",
    "# Group the train_data by 'store_nbr' and 'family'\n",
    "grouped_test = test_data.groupby(['store_nbr', 'family_encoded'])\n",
    "\n",
    "# Create an array of DataFrames\n",
    "dataframes_array_train = [group for _, group in grouped_train]\n",
    "\n",
    "# Create an array of DataFrames\n",
    "dataframes_array_test = [group for _, group in grouped_test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a row with the day 2012-12-25 to each DataFrame in dataframes_array_train\n",
    "for idx, df in enumerate(dataframes_array_train):\n",
    "    for i in range(2013, 2017):\n",
    "        new_row = pd.DataFrame({\n",
    "            'date': [pd.to_datetime(f'{i}-12-25')],\n",
    "            'store_nbr': [df['store_nbr'].iloc[0]],\n",
    "            'onpromotion': [0],\n",
    "            'cluster': [df['cluster'].iloc[0]],\n",
    "            'day_of_week': [df.loc[f'{i}-12-24', 'day_of_week'] + 1 if df.loc[f'{i}-12-24', 'day_of_week'] + 1 != 8 else 1],  # Assuming 1 for Monday, adjust as needed\n",
    "            'month': [12],\n",
    "            'family_encoded': [df['family_encoded'].iloc[0]],\n",
    "            'city_encoded': [df['city_encoded'].iloc[0]],\n",
    "            'state_encoded': [df['state_encoded'].iloc[0]],\n",
    "            'type_store_encoded': [df['type_store_encoded'].iloc[0]],\n",
    "            'type_holiday_encoded': [1],  # Assuming 1 for holiday, adjust as needed\n",
    "            'locale_encoded': [1],  # Assuming 1 for locale, adjust as needed\n",
    "            'transferred_encoded': [0],  # Assuming 0 for not transferred, adjust as needed\n",
    "            'dcoilwtico': [df.loc[f'{i}-12-24', 'dcoilwtico']],  # Assuming a default value, adjust as needed\n",
    "            'sales': [0]  # Assuming no sales, adjust as needed\n",
    "        })\n",
    "        new_row.set_index('date', inplace=True)\n",
    "        df = pd.concat([df, new_row])\n",
    "        df.sort_index(inplace=True)\n",
    "        #df.reset_index(inplace=True)\n",
    "        #df.set_index('date', inplace=True)\n",
    "    dataframes_array_train[idx] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_array_train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a complete date range\n",
    "complete_date_range = pd.date_range(start=train_data_start, end=train_data_end)\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = complete_date_range.difference(dataframes_array_train[0].loc[train_data_start:train_data_end].index)\n",
    "\n",
    "# Display missing dates\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only unique rows in each DataFrame in dataframes_array_train\n",
    "dataframes_array_train = [df.drop_duplicates(keep='first') for df in dataframes_array_train]\n",
    "\n",
    "# Fill missing dates in each DataFrame in dataframes_array_train\n",
    "for df in dataframes_array_train:\n",
    "#    complete_date_range = pd.date_range(start=df.index.min(), end=df.index.max())\n",
    "#    df = df.reindex(complete_date_range, method='ffill')\n",
    "#    df.sort_index(inplace=True)\n",
    "    df.asfreq('D', method='bfill')\n",
    "\n",
    "# Display the first few DataFrames in the array\n",
    "for df in dataframes_array_train[:3]:\n",
    "    print(df.head())\n",
    "\n",
    "for df in dataframes_array_test:\n",
    "#    complete_date_range = pd.date_range(start=df.index.min(), end=df.index.max())\n",
    "#    df = df.reindex(complete_date_range, method='ffill')\n",
    "#    df.sort_index(inplace=True)\n",
    "    df.asfreq('D', method='bfill')\n",
    "\n",
    "# Display the first few DataFrames in the array\n",
    "for df in dataframes_array_train[:3]:\n",
    "    print(df.head())\n",
    "\n",
    "# Create an array of DataFrames\n",
    "dataframes_array_test = [group for _, group in grouped_test]\n",
    "for df in dataframes_array_test[:3]:\n",
    "    print(df.head())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {}\n",
    "for idx, df in enumerate(dataframes_array_train):\n",
    "    target[idx] = df.loc[train_data_start:train_data_end, df.columns == 'sales']\n",
    "    \n",
    "target[0].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete target with 0 between test_data_start and test_data_end\n",
    "for idx, df in target.items():\n",
    "    complete_date_range = pd.date_range(start=test_data_start, end=test_data_end)\n",
    "    missing_dates = complete_date_range.difference(df.index)\n",
    "    for date in missing_dates:\n",
    "        df.loc[date] = 0\n",
    "    df.sort_index(inplace=True)\n",
    "    df.asfreq('D', method='bfill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes_array_train:\n",
    "    df.drop(columns=['sales'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(0, len(dataframes_array_train)):\n",
    "    dataframes_array_train[c] = pd.concat([dataframes_array_train[c], dataframes_array_test[c]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a complete date range\n",
    "complete_date_range = pd.date_range(start=train_data_start, end=train_data_end)\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = complete_date_range.difference(target[0].loc[train_data_start:train_data_end].index)\n",
    "\n",
    "# Display missing dates\n",
    "missing_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target[0].loc[train_data_start:test_data_end, 'sales'].head())\n",
    "print(target[0].loc[train_data_start:test_data_end, 'sales'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in target.values():\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.asfreq('D', method='bfill')\n",
    "    \n",
    "target[0].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = dataframes_array_train[0].loc[train_data_start:train_data_end, ['store_nbr', 'onpromotion', 'cluster', 'day_of_week', 'month', 'family_encoded', 'city_encoded', 'state_encoded', 'type_store_encoded', 'type_holiday_encoded', 'locale_encoded', 'transferred_encoded', 'dcoilwtico']]\n",
    "exog = exog.asfreq('D')\n",
    "target_test=target[0].loc[train_data_start:train_data_end, 'sales']\n",
    "target_test=target_test.asfreq('D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ForecasterSarimax(\n",
    "                 regressor = Sarimax(\n",
    "                                order          = (1, 1, 1),\n",
    "                                seasonal_order =(1, 1, 1, 12),\n",
    "                                maxiter        = 200\n",
    "                             )\n",
    "             )\n",
    "metric, predictions = backtesting_sarimax(\n",
    "                          forecaster            = forecaster,\n",
    "                          y                     = target_test,\n",
    "                          initial_train_size    = len(dataframes_array_train[0].loc[train_data_start:train_data_end])-1,\n",
    "                          fixed_train_size      = False,\n",
    "                          steps                 = len(dataframes_array_train[0].loc[test_data_start:test_data_end]),\n",
    "                          metric                = 'mean_absolute_error',\n",
    "                          refit                 = True,\n",
    "                          n_jobs                = \"auto\",\n",
    "                          suppress_warnings_fit = True,\n",
    "                          verbose               = True,\n",
    "                          show_progress         = True,\n",
    "                          exog=exog\n",
    "                      )\n",
    "display(metric)\n",
    "predictions.head(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ForecasterSarimax(\n",
    "                 regressor = Sarimax(\n",
    "                                order   = (1, 1, 1), # Placeholder replaced in the grid search\n",
    "                                maxiter = 500\n",
    "                             )\n",
    "             )\n",
    "\n",
    "param_grid = {\n",
    "    'order': [(0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (2, 1, 1)],\n",
    "    'seasonal_order': [(0, 0, 0, 0), (0, 1, 0, 12), (1, 1, 1, 12)],\n",
    "    'trend': [None, 'n']\n",
    "}\n",
    "results_grid = grid_search_sarimax(\n",
    "                   forecaster            = forecaster,\n",
    "                   y                     = target_test,\n",
    "                   param_grid            = param_grid,\n",
    "                   steps                 = len(dataframes_array_train[0].loc[test_data_start:test_data_end]),\n",
    "                   refit                 = True,\n",
    "                   metric                = 'mean_absolute_error',\n",
    "                   initial_train_size    = len(dataframes_array_train[0].loc[train_data_start:train_data_end])-1,\n",
    "                   fixed_train_size      = False,\n",
    "                   return_best           = True,\n",
    "                   n_jobs                = 'auto',\n",
    "                   suppress_warnings_fit = True,\n",
    "                   verbose               = False,\n",
    "                   show_progress         = True,\n",
    "                   exog                  = exog\n",
    "               )\n",
    "results_grid.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_test=target[1].loc[train_data_start:train_data_end, 'sales']\n",
    "target_test=target_test.asfreq('D')\n",
    "target_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(dataframes_array_train)):\n",
    "    exog = dataframes_array_train[i].loc[train_data_start:train_data_end, ['store_nbr', 'onpromotion', 'cluster', 'day_of_week', 'month', 'family_encoded', 'city_encoded', 'state_encoded', 'type_store_encoded', 'type_holiday_encoded', 'locale_encoded', 'transferred_encoded', 'dcoilwtico']]\n",
    "    exog = exog.asfreq('D').ffill()\n",
    "    target_test=target[i].loc[train_data_start:train_data_end, 'sales']\n",
    "    target_test=target_test.asfreq('D').ffill()  # Fill missing values\n",
    "    forecaster.fit(y=target_test, exog=exog)\n",
    "    # prediction\n",
    "    exog = dataframes_array_train[i].loc[test_data_start:test_data_end, ['store_nbr', 'onpromotion', 'cluster', 'day_of_week', 'month', 'family_encoded', 'city_encoded', 'state_encoded', 'type_store_encoded', 'type_holiday_encoded', 'locale_encoded', 'transferred_encoded', 'dcoilwtico']]\n",
    "    exog.index = pd.to_datetime(exog.index)\n",
    "    exog = exog.asfreq('D').ffill()\n",
    "    predicted_values = forecaster.predict(steps=len(dataframes_array_train[i].loc[test_data_start:test_data_end]), exog=exog)\n",
    "    predicted_values = pd.DataFrame({'store_nbr': [dataframes_array_train[i]['store_nbr'].iloc[0]], 'family_encoded': [dataframes_array_train[i]['family_encoded'].iloc[0]], 'sales': predicted_values}, index=exog.index)\n",
    "    final_df = pd.concat([final_df, predicted_values])\n",
    "    print (str(i) + ' of ' + str(len(dataframes_array_train)))\n",
    "    \n",
    "# Fit the model using train data with multiple series\n",
    "#exog = dataframes_array_train[0].loc[train_data_start:train_data_end, ['store_nbr', 'onpromotion', 'cluster', 'day_of_week', 'month', 'family_encoded', 'city_encoded', 'state_encoded', 'type_store_encoded', 'type_holiday_encoded', 'locale_encoded', 'transferred_encoded', 'dcoilwtico']]\n",
    "#exog = exog.asfreq('D')\n",
    "#target_test=target[0].loc[train_data_start:train_data_end, 'sales']\n",
    "#target_test=target_test.asfreq('D')\n",
    "#forecaster.fit(y=target_test, exog=exog)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the date column is in the index for both dataframes\n",
    "final_df.reset_index(inplace=True)\n",
    "full_test_df.reset_index(inplace=True)\n",
    "\n",
    "# Merge the dataframes on 'family_encoded', 'store_nbr', and 'date'\n",
    "merged_df = pd.merge(final_df, full_test_df, on=['family_encoded', 'store_nbr', 'date'], how='inner')\n",
    "\n",
    "# Display the merged dataframe\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=merged_df[['id', 'sales']]\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('RNNs/Tensorflow-Keras/Predictions/Store Sales - Time Series Forecasting/submission_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
